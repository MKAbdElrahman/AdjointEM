\documentclass{tufte-handout}
\title{ Adjoint Computational Electromagnetics}
\author[mohamedkamal]{Mohamed Kamal Abd Elrahman \\
6 October, Giza,\\ Egypt}

\usepackage{amsmath}  % extended mathematics
\usepackage{graphicx} % allow embedded images
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}} % set of paths to search for images
\usepackage{amsmath}  % extended mathematics
\usepackage{booktabs} % book-quality tables
\usepackage{units}    % non-stacked fractions and better unit spacing
\usepackage{multicol} % multiple column layout facilities
\usepackage{lipsum}   % filler text
\usepackage{fancyvrb} % extended verbatim environments
\fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

\begin{document}
	\maketitle
	\begin{abstract}
		\noindent
		 The adjoint method is used to  inverse design linear and nonlinear photonic devices. The purpose of this handout is give the reader an introduction to the topic.   
	\end{abstract}

	\section{Motivation}
	In normal electromagnetic simulation, we are given a material distribution represented by $\left( \epsilon(x,y,z), \mu(x,y,z)\right)  $ and a current density source $\mathbf{J(x,y,z)}$. Maxwell's equations are then solved using one of the computational techniques (FD, FEM, MoM) for the excited electromagnetic field $(\mathbf{H},\mathbf{E})$. For linear, isotropic, non magnetic and stationary media \sidenote[1]{In general $\epsilon$ and $\mu$ may be nonlinear anisotropic tensors. We restricted ourselves to linear isotropic media for simplicity and most material are nearly the case.} $\mathbf{B} = \mu_0 \mathbf{H}$ and $\mathbf{D} = \epsilon \mathbf{E}$. Maxwell's equations take the form 
	  \begin{subequations}
	  	\begin{align}
	  		\nabla \times \mathbf{E}  &= - \mu_0 \frac{\partial \mathbf{H}}{\partial t} \\
	    	\nabla \times \mathbf{H}  &= \mathbf{J} +  \epsilon \frac{\partial \mathbf{E}}{\partial t}
	  	\end{align}
	  \end{subequations}
 We are asked to maximize the scaler quantity $G(\mathbf{E},\mathbf{H})$ by varying the dielectric structure $\epsilon(x,y,z)$. $G$ is often a measure of device performance: for example, the power flowing in certain direction or mode conversion efficiency. This is an inverse design problem, we start by defining an objective $G$ and ask for the best material distribution  to achieve this objective. 
 
 The gradient quantity $\frac{dG}{d\epsilon}$ is  useful to compute. It could be used as a measure of robustness and sensitivity to any structure deformations like fabrication imperfections. In inverse design, it is the major quantity needed as it points in the direction of steepest ascent, from which we have an informed decision of how to update the dielectric structure.
 \begin{equation}
\epsilon \longleftarrow \epsilon + \alpha \frac{dG}{d\epsilon}
 \end{equation}
Where $\alpha$ is the step size in the gradient direction\sidenote[2]{Some algorithms attempt to
optimize the step size so that the step maximally increases $G$}. 	
 \\
 Assuming we have $n$ number of design points, to calculate the sensitivity $\frac{dG}{d\epsilon}$ there are different approaches which vary in complexity and accuracy. One way is by using a finite difference approximation $\frac{dG}{d\epsilon} \approx \frac{\Delta G}{ \Delta\epsilon}$. In this approach we need to carry $n$ number of forward simulations. For a typical $100 \times 100 \times 100$ $3$D  finite difference grid, this means $1000000$ simulations!. This approach also suffers from numerical discretization errors and the need to determine how small or large is $\Delta \epsilon$.  Automatic differentiation can be used to get the derivative exactly. However, there are are two modes in automatic differentiation. The automatic forward mode, like finite difference, needs $n$ number of forward simulations. In the automatic adjoint or reverse mode, two simulations are only needed regardless of the number of the design points. So, the adjoint method is by far much better in accuracy than finite difference and computational cost than automatic forward differentiation.
 \section{Problem Formulation}
 Before driving the adjoint equation, lets put Maxwell's in a matrix form:
 \begin{equation}
 \begin{bmatrix} 
 \mu_0 & 0 \\
 0 & -\epsilon
 \end{bmatrix}  \begin{bmatrix} 
 \frac{\partial \mathbf{H}}{\partial t}  \\
  \frac{\partial \mathbf{E}}{\partial t}
 \end{bmatrix} +\begin{bmatrix} 
 0 & \nabla \times \\
 \nabla \times & 0 
 \end{bmatrix}  \begin{bmatrix} 
 \mathbf{H}  \\
 \mathbf{E} 
 \end{bmatrix} = \begin{bmatrix} 
 0 \\
 \mathbf{J} 
 \end{bmatrix}
 \end{equation}
 Which could be written as
 \begin{equation}\label{forward_equation_sym}
C \frac{d \mathbf{X}}{dt} + D \mathbf{X} = \mathbf{S}
 \end{equation} 
 if the following definitions have been made $\mathbf{X} = \begin{bmatrix} 
 \mathbf{H}  \\
 \mathbf{E} 
 \end{bmatrix} $, $C = \begin{bmatrix} 
 \mu_0 & 0 \\
 0 & -\epsilon
 \end{bmatrix}$, $D = \begin{bmatrix} 
 0 & \nabla \times \\
 \nabla \times & 0 
 \end{bmatrix}  $ and $\mathbf{S} = \begin{bmatrix} 
 0 \\
 \mathbf{J} 
 \end{bmatrix} $
 
 The initial condition for equation(\ref{forward_equation_sym}) is \begin{equation}
I(\mathbf{X}(0),\epsilon) = \mathbf{0}
 \end{equation}
 Our optimization problem can  be then formulated as
  
  \begin{equation}\label{opt_problem}
  \begin{aligned}
  \max_{\epsilon} \quad & G(\epsilon) = \int^T_0 g(\mathbf{X},\epsilon,t) dt \\
  \textrm{s.t.} \quad & C \frac{d \mathbf{X}}{dt} + D \mathbf{X} - \mathbf{S} = 0\\
  &I(\mathbf{X}(0),\epsilon) = \mathbf{0}   \\
  \end{aligned}
  \end{equation}
 Where we have assumed our objective function can be written in this integral form $\int^T_0 g(\mathbf{X},\epsilon,t) dt$. 
 
 Equation (\ref{forward_equation_sym}) is  commonly solved by discretization  in space first and then solving the resulting system of Ordinary Differential Equations (ODEs). We can either start by replacing time derivatives with finite differences to arrive at a recursive set of spatial problems to be discretized. The discretization in space could be in done in real or spectral domains. Maxwell's equation could even be expanded in the natural complete eigen-basis of the problem in a coupled-mode-like fashion and inverse design could be done with very low dimensional matrices.   Among the numerical methods, the finite difference method, the finite element
 method, and the method of moments form the core of computational electromagnetics. One of our goals is to show a simple one dimensional example with code for each method  along with its adjoint formulation. Interested readers can directly add an adjoint solver on top their EM solvers very easily and start doing inverse design. 
 \section{Adjoint Method}
 In gradient based inverse design, we start with an initial dielectric profile $\epsilon^{0}(x,y,z)$  and then use local  gradient of the objective function to guide the search in device space \sidenote[3]{Additional constraints should be added to guarantee robustness and easy fabrication of the final discovered devices. Fabrication constraints play a key role in inverse design and we will talk about it more in a future separate handout.}. The gradient of the objective function $G$ in equation (\ref{opt_problem})
 \begin{equation}
 \frac{d G}{d\epsilon} = \int_{0}^{T} \left[  \frac{\partial g}{\partial \epsilon} +  \frac{\partial g}{\partial \mathbf{X}} \frac{\partial \mathbf{X}}{\partial \epsilon}\right] dt
 \end{equation}
 To evaluate the last integral, we need to calculate the term $ \frac{\partial \mathbf{X}}{\partial \epsilon}$. If the vector $\epsilon$ is $n$ dimensional, we need to repeat the simulation $n$ number of times. For each, we change the component $\epsilon_i$ by $\Delta \epsilon_i$, then the change in the objective function is determined. Finally, the slope is calculated by dividing the change in the objective function by $\Delta \epsilon_i$ . The whole point of adjoint method is to avoid calculating the derivative term $ \frac{\partial \mathbf{X}}{\partial \epsilon}$. Soon, we will discover that the adjoint method can magically determine the gradient at all design points with an additional cost of one extra simulation. The remaining terms inside the objective integral are not really problematic. $\frac{\partial g}{\partial \mathbf{X}}$ and $\frac{\partial g}{\partial \mathbf{X}}$ can be evaluated (efficiently and exactly) with a reverse mode  automatic differentiation package \sidenote[4]{There are tow levels of reverse mode differentiation (1) Differentiation of scaler function with respect to large dimensional vectors. (2) Efficient evaluation of PDE constrained objective function gradients using adjoint method. }. In Julia, the package "Nabla.jl" does a pretty good job in calculating these gradients.
 
 To eliminate the term $ \frac{\partial \mathbf{X}}{\partial \epsilon}$, the first step is to  add  multiples of $0$ to the objective function.
 \begin{equation}
G =  \int^T_0 g(\mathbf{X},\epsilon,t)   + \lambda^T \left( C \frac{d \mathbf{X}}{dt} + D \mathbf{X} - \mathbf{S}) \right) + \kappa^T\left( I(\mathbf{X}(0),\epsilon) \right) dt
 \end{equation}
 The vector  $\lambda$  is time dependent. $\kappa$  is another vector associated with the initial conditions.   The objective function has not changed after this modification, the equation is also the same  for any choice of $\lambda$ and $\kappa$. We will choose them in a way that eliminate the need for  the term   $ \frac{\partial \mathbf{X}}{\partial \epsilon}$ in the objective function gradient $\frac{dG}{d\epsilon}$. The vectors $\lambda$ and $\kappa$ are commonly called Lagrange multipliers and the modified objective function is the system Lagrangian that we seek to maximize. 
 
 Taking the total derivative of the Lagrangian,
 \begin{multline}\label{lag_grad}
\frac{d G}{d\epsilon} = \int_{0}^{T}\left( \left[  \frac{\partial g}{\partial \epsilon} +  \frac{\partial g}{\partial \mathbf{X}} \frac{\partial \mathbf{X}}{\partial \epsilon}\right] +  \lambda^T\left[ \frac{d C}{d \epsilon} \frac{d \mathbf{X}}{dt} + C \frac{\partial}{\partial \epsilon} \frac{d \mathbf{X}}{dt} + D \frac{\partial \mathbf{X}}{\partial \epsilon} \right]  \right) dt \\ +  \kappa^T  \left[ \frac{\partial I}{\partial \mathbf{X}(0)}  \frac{\partial \mathbf{X}(0)}{\partial \epsilon} + \frac{\partial I}{\partial \epsilon} \right] 
 \end{multline}

The problematic term  $\frac{\partial \mathbf{X}}{\partial \epsilon}$  is what we want to git rid off, however we have now its time derivative added. Integration by parts can help us reduce the time derivative to zero order.  
\begin{equation}\label{part_intg}
 \int_{0}^{T} \left(  \lambda^T C \frac{\partial}{\partial \epsilon}   \frac{d \mathbf{X}}{dt} \right) dt = \lambda^T C \frac{\partial \mathbf{X}}{\partial \epsilon}|^T_0 - \int_{0}^{T}
   \frac{ d\left(  \lambda^T C \right) }{dt} \frac{\partial \mathbf{X}}{\partial \epsilon} dt
\end{equation}
Substituting (\ref*{part_intg}) into (\ref*{lag_grad}) and collecting terms  in $\frac{\partial \mathbf{X}}{\partial \epsilon}$ and  $\frac{\partial \mathbf{X}(0)}{\partial \epsilon}$ yield

 \begin{multline}\label{lag_gradd}
 \frac{d G}{d\epsilon} = \int_{0}^{T} \left(  \left[  \frac{\partial g}{\partial \epsilon} + \lambda^T \frac{d C}{d \epsilon} \frac{d \mathbf{X}}{dt}  \right] +\left[  \frac{\partial g}{\partial \mathbf{X}}  + \lambda^T  D - \frac{ d\left(  \lambda^T C \right) }{dt}  \right]  \frac{\partial \mathbf{X}}{\partial \epsilon} \right)  dt +  \\      \left[ \kappa^T \frac{\partial I}{\partial \mathbf{X}(0)}  -\lambda^T C   \right]  \frac{\partial \mathbf{X}(0)}{\partial \epsilon} + \lambda^T C \frac{\partial \mathbf{X}(T)}{\partial \epsilon} + \kappa^T \frac{\partial I}{\partial \epsilon}
 \end{multline}
 We can get rid of $\frac{\partial \mathbf{X}}{\partial\epsilon}$  by choosing $\lambda^T$ to satisfy the following differential equation
\begin{equation}
 \frac{\partial g}{\partial \mathbf{X}}  + \lambda^T  D - \frac{ d\left(  \lambda^T C \right) }{dt} = 0
\end{equation}
We can get rid of  $\frac{\partial \mathbf{X}(0)}{d\epsilon}$  by choosing $\kappa^T = \lambda^T C \left[  \frac{\partial I}{\partial \mathbf{X}(0)}\right] ^{-1} $

If we also imposed the terminal condition $\lambda^T(T) C = 0$ and noting that in most cases, the initial conditions don't depend on the the material distribution $\frac{\partial I}{\partial \epsilon} = 0$.  The gradient of  objective function is then 
 \begin{equation}\label{lag_graddd}
 \frac{d G}{d\epsilon} = \int_{0}^{T} \left[  \frac{\partial g}{\partial \epsilon} + \lambda^T \frac{d C}{d \epsilon} \frac{d \mathbf{X}}{dt}  \right] dt 
 \end{equation}

The algorithm for computing $ \frac{d G}{d\epsilon}$ follows:
\begin{enumerate}
	\item Solve the forward problem for $\mathbf{X}$ from $0$ to $T$
	\begin{equation*}
	\begin{aligned}
 \quad & C \frac{d \mathbf{X}}{dt} + D \mathbf{X} - \mathbf{S} = 0\\
	&I(\mathbf{X}(0),\epsilon) = \mathbf{0}   \\
	\end{aligned}
	\end{equation*}
	\item Solve the adjoint problem for $\lambda$ from $T$ to $0$
	\begin{equation*}
	\begin{aligned}
	\quad & C^T \frac{d \lambda }{dt} - D^T \lambda = \left[ \frac{\partial g}{\partial \mathbf{X}}\right]^T \\
	&\lambda(T) = 0   \\
	\end{aligned}
	\end{equation*}
	\item Get the objective function gradient
	  \begin{equation}\label{lag_graddd}
	  \frac{d G}{d\epsilon} = \int_{0}^{T} \left[  \frac{\partial g}{\partial \epsilon} + \lambda^T \frac{d C}{d \epsilon} \frac{d \mathbf{X}}{dt}  \right] dt 
	  \end{equation}
\end{enumerate}
It worth having these points in mind:

\begin{itemize}
	\item The problem of calculating the gradient with respect to all design variables has been reduced  to solving one extra simulation backward in time.
	\item The adjoint equation is very similar to the forward equation, except the operators have been replaced by their adjoint (which is the same as t transpose for real operators). Thats why the problem is called the adjoint problem. For reciprocal systems, the matrices re equal to their adjoint and EM solvers already developed could be readily extended to solve the adjoint problem. 
	\item The adjoint problem has a different source term, which depends on the gradient of the forward problem solution. This would require storing the field $\mathbf{X}$ at all times. This is a big challenge for numerical methods like FDTD, which usually take long running simulation times.  
	\item The adjoint equation  could be converted to an initial value problem by substituting $t$ with $T-\tau$. 
	\item The objective integral needs the derivative of the forward field, which also requires the storage the storage  at all times.
\end{itemize}
\end{document}